"""
ECGå¿ƒç”µå›¾åˆ†æç³»ç»Ÿ - åŸºäºçœŸå®MIT-BIHæ•°æ®è®­ç»ƒ
ä½¿ç”¨çœŸå®æ•°æ®ï¼šå°æ³¢ç‰¹å¾ + æ—¶åŸŸç‰¹å¾ + MLP åˆ†ç±» + FPGA éƒ¨ç½²
"""

import argparse
import json
import os
import shutil
import sys
import textwrap
import time
from datetime import datetime
from pathlib import Path

import numpy as np
import pywt
from scipy import signal
import wfdb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.utils import class_weight
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras import regularizers
from tensorflow.keras.layers import (
    Dense,
    Dropout,
    Conv2D,
    MaxPooling2D,
    BatchNormalization,
    Flatten,
    SpatialDropout2D,
)
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.utils import Sequence
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')


CARDIAC_WAVELET_BANDS = [
    {
        'name': 'atrial_low',
        'range_hz': (0.5, 5.0),
        'description': 'ä½é¢‘æ®µ(0.5-5Hz)ï¼Œè¦†ç›–Pæ³¢/Tæ³¢ç­‰ç¼“æ…¢æˆåˆ†',
    },
    {
        'name': 'qrs_mid',
        'range_hz': (5.0, 15.0),
        'description': 'ä¸­é¢‘æ®µ(5-15Hz)ï¼Œèšç„¦äºQRSå¤åˆæ³¢ä¸»é¢‘',
    },
    {
        'name': 'high_freq',
        'range_hz': (15.0, 40.0),
        'description': 'é«˜é¢‘æ®µ(15-40Hz)ï¼Œæ•è·å®¤æ€§æ³¢ç¾¤/å¿«é€Ÿç—…ç†ç‰¹å¾',
    },
]

def _serialize_wavelet_bands(bands):
    return [
        {
            'name': band['name'],
            'range_hz': [float(band['range_hz'][0]), float(band['range_hz'][1])],
            'description': band.get('description', ''),
        }
        for band in bands
    ]


print("ğŸš€ ECGå¿ƒç”µå›¾åˆ†æç³»ç»Ÿ - åŸºäºçœŸå®MIT-BIHæ•°æ®è®­ç»ƒ")
print("=" * 70)

class MITBIHDataLoader:
    """MIT-BIHæ•°æ®åº“åŠ è½½å™¨"""
    
    def __init__(self, data_path='data'):
        self.data_path = data_path
        self.fs = 360  # MIT-BIHé‡‡æ ·é¢‘ç‡
        
        # MIT-BIHå¿ƒæ‹ç±»å‹æ˜ å°„åˆ°æ›´ä¸°å¯Œçš„å¿ƒå¾‹ç±»åˆ«ï¼Œå°½å¯èƒ½è¦†ç›–æ›´å¤šå¿ƒå¾‹å¤±å¸¸
        self.symbol_to_class = {
            'N': 'Normal',
            'L': 'LeftBundleBranchBlock',
            'R': 'RightBundleBranchBlock',
            'B': 'BundleBranchBlock',
            'A': 'AtrialPremature',
            'a': 'AberratedAtrialPremature',
            'J': 'JunctionalPremature',
            'S': 'SupraventricularPremature',
            'e': 'AtrialEscape',
            'j': 'JunctionalEscape',
            'V': 'PrematureVentricular',
            'E': 'VentricularEscape',
            'F': 'Fusion',
            '/': 'Paced',
            'f': 'FusionPaced',
            'x': 'NonConductedPWave',
            'Q': 'Unclassifiable',
            '|': 'IsolatedQRSLike',
            '!': 'VentricularFlutter',
            '[': 'VentricularFlutterStart',
            ']': 'VentricularFlutterEnd',
            'p': 'PacedPremature',
            't': 'TWaveAbnormality'
        }

        self.class_names = sorted(set(self.symbol_to_class.values()))
        
    def get_available_records(self):
        """è·å–å¯ç”¨çš„è®°å½•æ–‡ä»¶"""
        records = []
        for file in os.listdir(self.data_path):
            if file.endswith('.dat'):
                record_id = file.replace('.dat', '')
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯¹åº”çš„.heaå’Œ.atræ–‡ä»¶
                if (os.path.exists(f'{self.data_path}/{record_id}.hea') and 
                    os.path.exists(f'{self.data_path}/{record_id}.atr')):
                    records.append(record_id)
        return sorted(records)
    
    def load_record(self, record_id):
        """åŠ è½½å•ä¸ªè®°å½•"""
        try:
            # è¯»å–ä¿¡å·å’Œæ ‡æ³¨
            record = wfdb.rdrecord(f'{self.data_path}/{record_id}')
            annotation = wfdb.rdann(f'{self.data_path}/{record_id}', 'atr')
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯¼è”
            signal_data = record.p_signal[:, 0]
            
            return signal_data, annotation
        except Exception as e:
            print(f"   âš ï¸  åŠ è½½è®°å½• {record_id} å¤±è´¥: {e}")
            return None, None
    
    def extract_beats_from_record(self, signal_data, annotation, pre_samples=100, post_samples=199):
        """ä»è®°å½•ä¸­æå–å¿ƒæ‹"""
        beats = []
        labels = []
        
        for i, (sample, symbol) in enumerate(zip(annotation.sample, annotation.symbol)):
            # è·³è¿‡éå¿ƒæ‹æ ‡æ³¨
            if symbol not in self.symbol_to_class:
                continue
                
            # æå–å¿ƒæ‹ç‰‡æ®µ
            start = sample - pre_samples
            end = sample + post_samples + 1
            
            if start >= 0 and end < len(signal_data):
                beat = signal_data[start:end]
                if len(beat) == 300:  # ç¡®ä¿é•¿åº¦ä¸€è‡´
                    beats.append(beat)
                    labels.append(self.symbol_to_class[symbol])
        
        return np.array(beats), np.array(labels)
    
    def load_all_data(self, max_records=20):
        """åŠ è½½æ‰€æœ‰æ•°æ®"""
        print("ğŸ“Š åŠ è½½MIT-BIHæ•°æ®åº“...")
        
        records = self.get_available_records()[:max_records]
        print(f"   ğŸ“ æ‰¾åˆ° {len(records)} ä¸ªè®°å½•æ–‡ä»¶")
        
        all_beats = []
        all_labels = []
        
        for i, record_id in enumerate(records):
            print(f"   ğŸ“– åŠ è½½è®°å½• {record_id} ({i+1}/{len(records)})")
            
            signal_data, annotation = self.load_record(record_id)
            if signal_data is not None:
                beats, labels = self.extract_beats_from_record(signal_data, annotation)
                
                if len(beats) > 0:
                    all_beats.append(beats)
                    all_labels.append(labels)
                    print(f"      âœ… æå–äº† {len(beats)} ä¸ªå¿ƒæ‹")
        
        if all_beats:
            all_beats = np.vstack(all_beats)
            all_labels = np.hstack(all_labels)

            # é‡æ–°æ˜ å°„æ ‡ç­¾åˆ°è¿ç»­èŒƒå›´ 0-(n-1)
            unique_label_names = sorted(np.unique(all_labels))
            label_mapping = {label_name: idx for idx, label_name in enumerate(unique_label_names)}
            numeric_labels = np.array([label_mapping[label] for label in all_labels], dtype=np.int32)

            # æ›´æ–°å¯ç”¨ç±»åˆ«
            self.class_names = unique_label_names

            print(f"   âœ… æ€»å…±åŠ è½½äº† {len(all_beats)} ä¸ªå¿ƒæ‹")
            unique, counts = np.unique(numeric_labels, return_counts=True)
            for label_idx, count in zip(unique, counts):
                class_name = unique_label_names[label_idx]
                print(f"      ç±»åˆ« {class_name}: {count} ä¸ª")

            return all_beats, numeric_labels, label_mapping, unique_label_names
        else:
            raise Exception("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®")

def preprocess_signal(signal_data, fs=360):
    """ä¿¡å·é¢„å¤„ç†"""
    # å¸¦é€šæ»¤æ³¢ (0.5-40Hz)
    nyq = 0.5 * fs
    low = 0.5 / nyq
    high = 40 / nyq
    b, a = signal.butter(4, [low, high], btype='band')
    filtered = signal.filtfilt(b, a, signal_data)
    
    # å°æ³¢é™å™ª
    coeffs = pywt.wavedec(filtered, 'db4', level=6)
    sigma = np.median(np.abs(coeffs[-1])) / 0.6745
    threshold = sigma * np.sqrt(2 * np.log(len(filtered)))
    coeffs_thresh = coeffs.copy()
    coeffs_thresh[1:] = [pywt.threshold(detail, threshold, 'soft') 
                        for detail in coeffs_thresh[1:]]
    denoised = pywt.waverec(coeffs_thresh, 'db4')
    
    return denoised

def extract_wavelet_features(beat, wavelet='db4', levels=5):
    """æå–å°æ³¢ç‰¹å¾"""
    coeffs = pywt.wavedec(beat, wavelet, level=levels)
    
    features = []
    for coeff in coeffs:
        features.extend([
            np.mean(coeff),
            np.std(coeff),
            np.max(coeff),
            np.min(coeff),
            np.sum(coeff**2),  # èƒ½é‡
            np.sum(np.abs(coeff)),  # ç»å¯¹å€¼å’Œ
        ])
    
    return features

def extract_time_features(beat):
    """æå–æ—¶åŸŸç‰¹å¾"""
    features = [
        np.mean(beat),
        np.std(beat),
        np.max(beat),
        np.min(beat),
        np.max(beat) - np.min(beat),  # å³°å³°å€¼
        np.sum(beat**2),  # èƒ½é‡
        np.sqrt(np.mean(beat**2)),  # RMS
        np.sum(np.diff(np.sign(beat)) != 0),  # è¿‡é›¶ç‚¹
        np.mean(np.abs(np.diff(beat))),  # å¹³å‡ç»å¯¹å·®åˆ†
        np.std(np.diff(beat)),  # å·®åˆ†æ ‡å‡†å·®
    ]
    return features


def _build_frequency_masks(scales, wavelet, fs, bands):
    """æ ¹æ®é¢‘æ®µå®šä¹‰ç”Ÿæˆå°ºåº¦æ©ç """

    freqs = pywt.scale2frequency(wavelet, scales)
    freqs = freqs * fs

    masks = []
    for band in bands:
        low, high = band['range_hz']
        mask = (freqs >= low) & (freqs < high)

        # è‹¥ä¸¥æ ¼èŒƒå›´å†…æ— å°ºåº¦ï¼Œæ”¾å®½ 10% ä»¥ç¡®ä¿è‡³å°‘åŒ…å«ä¸€ä¸ªå°ºåº¦
        if not np.any(mask):
            tolerance = max(0.5, 0.1 * (high - low))
            mask = (freqs >= max(0.0, low - tolerance)) & (freqs < high + tolerance)

        masks.append(mask.astype(np.float32))

    return np.stack(masks, axis=0)


def create_wavelet_tensors(
    beats,
    wavelet='morl',
    scales=None,
    output_format='2d',
    fs=360,
    channel_strategy='cardiac_band',
    cache_dir='outputs/cache',
    dtype=np.float16,
):
    """æ ¹æ®å¿ƒæ‹ç”Ÿæˆå°æ³¢å¼ é‡å¹¶å¯é€‰è½ç›˜ç¼“å­˜ä»¥é™ä½å†…å­˜å³°å€¼

    Args:
        beats (np.ndarray): å¿ƒæ‹é›†åˆï¼Œå½¢çŠ¶ä¸º [N, T]
        wavelet (str): å°æ³¢åŸºç±»å‹
        scales (list or np.ndarray, optional): å°æ³¢å°ºåº¦
        output_format (str): "2d" è¿”å› [N, H, W, C] å¼ é‡, "sequence" è¿”å› [N, T, C]
        fs (int): é‡‡æ ·é¢‘ç‡
        channel_strategy (str): ç”Ÿæˆé€šé“çš„ç­–ç•¥ï¼Œé»˜è®¤ä¸ºæ ¹æ®P/QRS/Té¢‘æ®µç”Ÿæˆå¤šé€šé“
        cache_dir (str or Path): è‹¥æä¾›åˆ™ä½¿ç”¨å†…å­˜æ˜ å°„æ–‡ä»¶ç¼“å­˜ç»“æœï¼Œé¿å…ä¸€æ¬¡æ€§å æ»¡å†…å­˜
        dtype (np.dtype): å­˜å‚¨å°æ³¢å¼ é‡çš„ç²¾åº¦ï¼Œé»˜è®¤ä¸º float16 ä»¥å‹ç¼©ç£ç›˜/å†…å­˜å ç”¨

    Returns:
        tuple[np.memmap, dict]: ç”Ÿæˆçš„å°æ³¢å¼ é‡å†…å­˜æ˜ å°„åŠç›¸å…³å…ƒä¿¡æ¯
    """

    if scales is None:
        scales = np.arange(1, 65)

    if channel_strategy not in {'cardiac_band', 'single'}:
        raise ValueError("channel_strategy must be 'cardiac_band' or 'single'")

    beats = np.asarray(beats, dtype=np.float32)
    if beats.ndim != 2:
        raise ValueError("beats å¿…é¡»æ˜¯å½¢çŠ¶ä¸º [N, T] çš„äºŒç»´æ•°ç»„")

    num_samples, signal_length = beats.shape
    if num_samples == 0:
        raise ValueError("ä¼ å…¥çš„å¿ƒæ‹æ•°é‡ä¸º0ï¼Œæ— æ³•ç”Ÿæˆå°æ³¢å¼ é‡")

    frequency_masks = None
    if channel_strategy == 'cardiac_band':
        frequency_masks = _build_frequency_masks(scales, wavelet, fs, CARDIAC_WAVELET_BANDS)

    num_channels = len(frequency_masks) if frequency_masks is not None else 1
    dtype = np.dtype(dtype)

    memmap = None
    memmap_path = None
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    target_shape = (num_samples, len(scales), signal_length, num_channels)

    if cache_dir is not None:
        cache_dir = Path(cache_dir)
        cache_dir.mkdir(parents=True, exist_ok=True)
        memmap_path = cache_dir / f'wavelet_tensors_{timestamp}.dat'
        memmap = np.memmap(memmap_path, mode='w+', dtype=dtype, shape=target_shape)
        tensors = memmap
    else:
        tensors = np.empty(target_shape, dtype=dtype)

    channel_sum = np.zeros(num_channels, dtype=np.float64)
    channel_sq_sum = np.zeros(num_channels, dtype=np.float64)
    channel_counts = np.zeros(num_channels, dtype=np.int64)

    for idx, beat in enumerate(beats):
        if idx % 1000 == 0:
            print(f"   ç”Ÿæˆå°æ³¢å¼ é‡ {idx + 1}/{len(beats)}")

        beat_norm = (beat - np.mean(beat)) / (np.std(beat) + 1e-8)

        coefficients, _ = pywt.cwt(beat_norm, scales, wavelet)
        scalogram = np.log1p(np.abs(coefficients).astype(np.float32))

        if channel_strategy == 'cardiac_band' and frequency_masks is not None:
            stacked = np.zeros((len(scales), signal_length, num_channels), dtype=np.float32)
            for channel_idx, mask in enumerate(frequency_masks):
                active_rows = mask > 0
                band_slice = scalogram[active_rows, :]
                if band_slice.size == 0:
                    continue
                channel_sum[channel_idx] += float(np.sum(band_slice))
                channel_sq_sum[channel_idx] += float(np.sum(np.square(band_slice)))
                channel_counts[channel_idx] += band_slice.size
                stacked[:, :, channel_idx] = scalogram * mask[:, np.newaxis]
        else:
            stacked = scalogram[..., np.newaxis]
            channel_sum[0] += float(np.sum(scalogram))
            channel_sq_sum[0] += float(np.sum(np.square(scalogram)))
            channel_counts[0] += scalogram.size

        tensors[idx] = stacked.astype(dtype, copy=False)

    clip_min, clip_max = -5.0, 5.0
    channel_mean = np.divide(
        channel_sum,
        channel_counts,
        out=np.zeros_like(channel_sum),
        where=channel_counts > 0,
    )
    channel_var = np.divide(
        channel_sq_sum,
        channel_counts,
        out=np.zeros_like(channel_sq_sum),
        where=channel_counts > 0,
    ) - np.square(channel_mean)
    channel_var = np.maximum(channel_var, 1e-6)
    channel_std = np.sqrt(channel_var)

    if isinstance(tensors, np.memmap):
        for idx in range(num_samples):
            sample = np.asarray(tensors[idx], dtype=np.float32)
            normalized = (sample - channel_mean.reshape((1, 1, num_channels))) / channel_std.reshape((1, 1, num_channels))
            normalized = np.clip(normalized, clip_min, clip_max)
            tensors[idx] = normalized.astype(dtype, copy=False)

        tensors.flush()
        info = {
            'path': str(memmap_path),
            'shape': list(target_shape),
            'dtype': tensors.dtype.str,
            'timestamp': timestamp,
            'wavelet': wavelet,
            'channel_strategy': channel_strategy,
            'normalization': {
                'type': 'log1p_zscore',
                'clip_range': [clip_min, clip_max],
                'channel_mean': channel_mean.astype(np.float32).tolist(),
                'channel_std': channel_std.astype(np.float32).tolist(),
            },
        }
        readonly_memmap = np.memmap(memmap_path, mode='r', dtype=dtype, shape=target_shape)
        return readonly_memmap, info

    tensors = tensors.astype(np.float32)
    tensors = (tensors - channel_mean.reshape((1, 1, 1, num_channels))) / channel_std.reshape((1, 1, 1, num_channels))
    tensors = np.clip(tensors, clip_min, clip_max)

    info = {
        'shape': list(target_shape),
        'dtype': tensors.dtype.str,
        'wavelet': wavelet,
        'channel_strategy': channel_strategy,
        'normalization': {
            'type': 'log1p_zscore',
            'clip_range': [clip_min, clip_max],
            'channel_mean': channel_mean.astype(np.float32).tolist(),
            'channel_std': channel_std.astype(np.float32).tolist(),
        },
    }

    if output_format == '2d':
        return tensors, info
    if output_format == 'sequence':
        reshaped = tensors.reshape(tensors.shape[0], tensors.shape[2], -1)
        seq_info = info.copy()
        seq_info['shape'] = list(reshaped.shape)
        seq_info['dtype'] = reshaped.dtype.str
        return reshaped.astype(np.float32), seq_info

    raise ValueError("output_format must be '2d' or 'sequence'")


def filter_rare_classes(beats, labels, label_mapping, class_names, min_count=2):
    """ç§»é™¤æ ·æœ¬æ•°é‡ä¸è¶³çš„ç±»åˆ«ä»¥ä¿è¯åˆ†å±‚æŠ½æ ·çš„ç¨³å®šæ€§"""

    unique_labels, counts = np.unique(labels, return_counts=True)
    if not len(unique_labels):
        raise ValueError("æ ‡ç­¾æ•°ç»„ä¸ºç©ºï¼Œæ— æ³•ç»§ç»­è®­ç»ƒ")

    inverse_mapping = {index: name for name, index in label_mapping.items()}

    rare_indices = [label for label, count in zip(unique_labels, counts) if count < min_count]
    if not rare_indices:
        return beats, labels, label_mapping, class_names, {}

    rare_summary = {inverse_mapping[idx]: int(count)
                    for idx, count in zip(unique_labels, counts)
                    if idx in rare_indices}

    keep_indices = [label for label, count in zip(unique_labels, counts) if count >= min_count]
    if not keep_indices:
        raise ValueError(
            "æ‰€æœ‰ç±»åˆ«çš„æ ·æœ¬æ•°é‡éƒ½ä¸è¶³ä»¥è¿›è¡Œè®­ç»ƒï¼Œè¯·æ£€æŸ¥æ•°æ®é›†æˆ–æ”¾å®½æœ€å°é˜ˆå€¼ã€‚"
        )

    keep_mask = np.isin(labels, keep_indices)
    filtered_beats = beats[keep_mask]
    filtered_labels = labels[keep_mask]

    keep_indices_sorted = sorted(keep_indices)
    reindex_map = {old_idx: new_idx for new_idx, old_idx in enumerate(keep_indices_sorted)}

    remapped_labels = np.array([reindex_map[idx] for idx in filtered_labels], dtype=np.int32)
    new_class_names = [inverse_mapping[idx] for idx in keep_indices_sorted]
    new_label_mapping = {name: reindex_map[idx]
                         for name, idx in label_mapping.items()
                         if idx in reindex_map}

    return filtered_beats, remapped_labels, new_label_mapping, new_class_names, rare_summary


def compute_class_distribution(labels, class_names):
    """ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒï¼Œè¿”å›ç±»åˆ«ååˆ°æ•°é‡çš„æ˜ å°„"""

    unique_labels, counts = np.unique(labels, return_counts=True)
    distribution = {}
    for label, count in zip(unique_labels, counts):
        if 0 <= label < len(class_names):
            distribution[class_names[label]] = int(count)
        else:
            distribution[str(label)] = int(count)
    return distribution


def stratified_train_val_test_split(X, y, test_size=0.2, val_size=0.1, random_state=42):
    """åˆ†å±‚æ‹†åˆ†è®­ç»ƒ/éªŒè¯/æµ‹è¯•é›†"""

    if not 0 < test_size < 1:
        raise ValueError("test_size å¿…é¡»åœ¨ (0, 1) èŒƒå›´å†…")
    if not 0 < val_size < 1:
        raise ValueError("val_size å¿…é¡»åœ¨ (0, 1) èŒƒå›´å†…")

    X_temp, X_test, y_temp, y_test = train_test_split(
        X,
        y,
        test_size=test_size,
        random_state=random_state,
        stratify=y,
    )

    remaining = 1.0 - test_size
    adjusted_val = val_size / remaining

    X_train, X_val, y_train, y_val = train_test_split(
        X_temp,
        y_temp,
        test_size=adjusted_val,
        random_state=random_state,
        stratify=y_temp,
    )

    return X_train, X_val, X_test, y_train, y_val, y_test


def _augment_cwt_samples(samples, rng, noise_std=0.01, max_shift=4, scale_range=(0.9, 1.1)):
    """åœ¨è¿‡é‡‡æ ·é˜¶æ®µå¯¹å°æ³¢å¼ é‡è¿›è¡Œè½»é‡æ‰°åŠ¨"""

    augmented = samples.astype(np.float32).copy()

    if max_shift > 0:
        shifts = rng.integers(-max_shift, max_shift + 1, size=samples.shape[0])
        for i, shift in enumerate(shifts):
            if shift == 0:
                continue
            augmented[i] = np.roll(augmented[i], shift=shift, axis=1)

    if scale_range is not None:
        low, high = scale_range
        scales = rng.uniform(low, high, size=(samples.shape[0], 1, 1, samples.shape[-1]))
        augmented = augmented * scales.astype(np.float32)

    if noise_std > 0:
        noise = rng.normal(loc=0.0, scale=noise_std, size=samples.shape).astype(np.float32)
        augmented += noise

    return np.clip(augmented, 0.0, 1.0)


def rebalance_training_data(
    X,
    y,
    min_samples_per_class=32,
    noise_std=0.01,
    random_state=42,
    adaptive_target=True,
):
    """å¯¹è®­ç»ƒæ•°æ®è¿›è¡Œè¿‡é‡‡æ ·ï¼Œé¿å…æç«¯ç±»ä¸å¹³è¡¡"""

    unique_labels, counts = np.unique(y, return_counts=True)
    augmentations = {}
    augmented_sets = [X]
    augmented_labels = [y]

    rng = np.random.default_rng(random_state)

    target_per_class = min_samples_per_class
    if adaptive_target and counts.size:
        percentile_75 = np.percentile(counts, 75)
        target_per_class = max(min_samples_per_class, int(np.ceil(percentile_75)))

    for label, count in zip(unique_labels, counts):
        if count >= target_per_class:
            continue

        label_indices = np.where(y == label)[0]
        if label_indices.size == 0:
            continue

        needed = int(target_per_class - count)
        sampled_indices = rng.choice(label_indices, size=needed, replace=True)
        samples = X[sampled_indices]
        samples = _augment_cwt_samples(samples, rng, noise_std=noise_std)

        augmented_sets.append(samples)
        augmented_labels.append(np.full(needed, label, dtype=y.dtype))
        augmentations[int(label)] = {
            'original': int(count),
            'added': int(needed),
            'final': int(count + needed),
            'target': int(target_per_class),
        }

    if len(augmented_sets) == 1:
        return X, y, augmentations

    X_augmented = np.concatenate(augmented_sets, axis=0)
    y_augmented = np.concatenate(augmented_labels, axis=0)

    permutation = rng.permutation(len(y_augmented))
    return X_augmented[permutation], y_augmented[permutation], augmentations


class WaveletTensorSequence(Sequence):
    """åŸºäºå†…å­˜æ˜ å°„å°æ³¢å¼ é‡çš„æ‰¹é‡åŠ è½½å™¨"""

    def __init__(
        self,
        memmap_info,
        indices,
        labels,
        batch_size=32,
        shuffle=False,
        augment=False,
        noise_std=0.01,
        max_shift=4,
        scale_range=(0.9, 1.1),
        oversample_target=None,
        adaptive_target=True,
        seed=42,
    ):
        if not isinstance(memmap_info, dict):
            raise ValueError('memmap_info å¿…é¡»ä¸ºå­—å…¸ç±»å‹')

        self.memmap_path = memmap_info.get('path')
        self.memmap_shape = tuple(memmap_info.get('shape', ()))
        self.memmap_dtype = np.dtype(memmap_info.get('dtype', np.float16))

        if not self.memmap_path or not self.memmap_shape:
            raise ValueError('memmap_info ç¼ºå°‘è·¯å¾„æˆ–å½¢çŠ¶ä¿¡æ¯')

        self._memmap = np.memmap(
            self.memmap_path,
            mode='r',
            dtype=self.memmap_dtype,
            shape=self.memmap_shape,
        )

        self.base_indices = np.asarray(indices, dtype=np.int64)
        self.base_labels = np.asarray(labels, dtype=np.int32)

        if self.base_indices.size != self.base_labels.size:
            raise ValueError('indices ä¸ labels é•¿åº¦ä¸ä¸€è‡´')

        self.batch_size = int(batch_size)
        self.shuffle = bool(shuffle)
        self.augment = bool(augment)
        self.noise_std = float(noise_std)
        self.max_shift = int(max_shift)
        self.scale_range = scale_range
        self.adaptive_target = bool(adaptive_target)
        self.rng = np.random.default_rng(seed)
        self.sample_shape = tuple(self.memmap_shape[1:])

        self.indices = self.base_indices.copy()
        self.labels = self.base_labels.copy()
        self.augmentations = {}
        self.min_samples_per_class = int(oversample_target) if oversample_target is not None else None

        if oversample_target is not None and self.base_labels.size:
            self._apply_oversampling(int(oversample_target))

        self.on_epoch_end()

    def _apply_oversampling(self, min_samples_per_class):
        unique_labels, counts = np.unique(self.base_labels, return_counts=True)
        if unique_labels.size == 0:
            return

        target = min_samples_per_class
        if self.adaptive_target and counts.size:
            percentile_75 = np.percentile(counts, 75)
            target = max(min_samples_per_class, int(np.ceil(percentile_75)))

        augmented_indices = [self.base_indices]
        augmented_labels = [self.base_labels]
        augmentations = {}

        for label, count in zip(unique_labels, counts):
            if count >= target:
                continue

            mask = self.base_labels == label
            label_indices = self.base_indices[mask]
            if label_indices.size == 0:
                continue

            needed = int(target - count)
            sampled = self.rng.choice(label_indices, size=needed, replace=True)

            augmented_indices.append(sampled.astype(np.int64))
            augmented_labels.append(np.full(needed, label, dtype=np.int32))
            augmentations[int(label)] = {
                'original': int(count),
                'added': int(needed),
                'final': int(count + needed),
                'target': int(target),
            }

        if len(augmented_indices) > 1:
            self.indices = np.concatenate(augmented_indices)
            self.labels = np.concatenate(augmented_labels)
        else:
            self.indices = self.base_indices.copy()
            self.labels = self.base_labels.copy()

        self.augmentations = augmentations

    def __len__(self):
        if self.batch_size <= 0:
            raise ValueError('batch_size å¿…é¡»å¤§äº0')
        return int(np.ceil(len(self.indices) / self.batch_size))

    def __getitem__(self, idx):
        start = idx * self.batch_size
        end = min(start + self.batch_size, len(self.indices))
        batch_indices = self.indices[start:end]
        batch_labels = self.labels[start:end]

        batch = np.asarray(self._memmap[batch_indices], dtype=np.float32)
        if self.augment:
            batch = _augment_cwt_samples(
                batch,
                rng=self.rng,
                noise_std=self.noise_std,
                max_shift=self.max_shift,
                scale_range=self.scale_range,
            )

        return batch, batch_labels

    def on_epoch_end(self):
        if self.shuffle and len(self.indices) > 1:
            permutation = self.rng.permutation(len(self.indices))
            self.indices = self.indices[permutation]
            self.labels = self.labels[permutation]

    def get_base_distribution(self, class_names):
        return compute_class_distribution(self.base_labels, class_names)

    def get_balanced_distribution(self, class_names):
        return compute_class_distribution(self.labels, class_names)

    def get_effective_size(self):
        return int(len(self.indices))

    def get_original_size(self):
        return int(len(self.base_labels))

    def get_augmentations(self):
        return self.augmentations

    def get_labels(self):
        return self.base_labels.copy()

    def get_numpy_subset(self, subset_indices):
        subset_indices = np.asarray(subset_indices, dtype=np.int64)
        return np.asarray(self._memmap[subset_indices], dtype=np.float32)


def sample_wavelet_representatives(memmap_info, indices, sample_size=512, seed=42):
    """ä»å°æ³¢å¼ é‡å†…å­˜æ˜ å°„ä¸­æŠ½å–ä»£è¡¨æ€§æ ·æœ¬ï¼Œç”¨äºé‡åŒ–æˆ–è°ƒè¯•"""

    if not isinstance(memmap_info, dict):
        raise ValueError('memmap_info å¿…é¡»ä¸ºå­—å…¸ç±»å‹')

    indices = np.asarray(indices, dtype=np.int64)
    if indices.size == 0:
        return np.empty((0,) + tuple(memmap_info.get('shape', (0,))[1:]), dtype=np.float32)

    rng = np.random.default_rng(seed)
    sample_size = min(int(sample_size), indices.size)
    chosen = rng.choice(indices, size=sample_size, replace=False)

    memmap = np.memmap(
        memmap_info['path'],
        mode='r',
        dtype=np.dtype(memmap_info.get('dtype', np.float16)),
        shape=tuple(memmap_info.get('shape', ())),
    )
    samples = np.asarray(memmap[chosen], dtype=np.float32)

    return samples


def extract_all_features(beats):
    """æå–æ‰€æœ‰ç‰¹å¾"""
    print("ğŸ”§ æå–ç‰¹å¾...")
    
    all_features = []
    
    for i, beat in enumerate(beats):
        if i % 1000 == 0:
            print(f"   å¤„ç†ç¬¬ {i+1}/{len(beats)} ä¸ªå¿ƒæ‹")
        
        # å½’ä¸€åŒ–
        beat_norm = (beat - np.mean(beat)) / (np.std(beat) + 1e-8)
        
        # å°æ³¢ç‰¹å¾
        wavelet_features = extract_wavelet_features(beat_norm)
        
        # æ—¶åŸŸç‰¹å¾
        time_features = extract_time_features(beat_norm)
        
        # åˆå¹¶ç‰¹å¾
        combined_features = wavelet_features + time_features
        all_features.append(combined_features)
    
    return np.array(all_features)

def build_mlp_model(input_dim, num_classes):
    """æ„å»ºMLPï¼ˆå¤šå±‚æ„ŸçŸ¥æœºï¼‰æ¨¡å‹ - åŸºäº46ç»´ç‰¹å¾å‘é‡"""
    model = Sequential([
        Dense(128, activation='relu', input_shape=(input_dim,)),
        Dropout(0.3),
        Dense(64, activation='relu'),
        Dropout(0.3),
        Dense(32, activation='relu'),
        Dropout(0.2),
        Dense(num_classes, activation='softmax')
    ])
    
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='sparse_categorical_crossentropy',
        metrics=['accuracy']
    )
    
    return model


def build_cnn_model(input_shape, num_classes, learning_rate=3e-4, weight_decay=1e-4):
    """æ„å»ºæ­£åˆ™åŒ–å¢å¼ºçš„CWT-CNNæ¨¡å‹ï¼Œæé«˜ç¨³å®šæ€§ä¸æ³›åŒ–èƒ½åŠ›"""

    kernel_regularizer = regularizers.l2(weight_decay)

    def conv_block(x, filters, kernel_size=(3, 3), pool=True, dropout_rate=0.2, dilation_rate=1):
        x = Conv2D(
            filters,
            kernel_size,
            padding='same',
            dilation_rate=dilation_rate,
            use_bias=False,
            kernel_initializer='he_normal',
            kernel_regularizer=kernel_regularizer,
        )(x)
        x = BatchNormalization()(x)
        x = tf.keras.layers.Activation('relu')(x)
        x = SpatialDropout2D(dropout_rate)(x)
        if pool:
            x = MaxPooling2D((2, 2))(x)
        return x

    inputs = tf.keras.layers.Input(shape=input_shape)
    x = conv_block(inputs, 48, kernel_size=(5, 5), dropout_rate=0.1)
    x = conv_block(x, 96, kernel_size=(3, 3), dropout_rate=0.15, dilation_rate=2)
    x = conv_block(x, 160, kernel_size=(3, 3), dropout_rate=0.2)
    x = Flatten()(x)
    x = Dense(
        256,
        activation='relu',
        kernel_initializer='he_normal',
        kernel_regularizer=kernel_regularizer,
    )(x)
    x = Dropout(0.45)(x)
    x = Dense(
        128,
        activation='relu',
        kernel_initializer='he_normal',
        kernel_regularizer=kernel_regularizer,
    )(x)
    x = Dropout(0.35)(x)

    outputs = Dense(num_classes, activation='softmax')(x)

    model = tf.keras.Model(inputs=inputs, outputs=outputs, name='wavelet_regularized_cnn')

    optimizer = Adam(learning_rate=learning_rate)
    loss = tf.keras.losses.SparseCategoricalCrossentropy(label_smoothing=0.1)

    model.compile(
        optimizer=optimizer,
        loss=loss,
        metrics=[
            'accuracy',
            tf.keras.metrics.SparseTopKCategoricalAccuracy(name='top3_accuracy', k=3),
        ]
    )

    return model

def train_model(X_train, X_test, y_train, y_test, class_names=None):
    """è®­ç»ƒæ¨¡å‹"""
    print("ğŸ§  è®­ç»ƒMLPæ¨¡å‹...")

    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # æ„å»ºæ¨¡å‹
    model = build_mlp_model(X_train.shape[1], len(np.unique(y_train)))
    
    # è®­ç»ƒ
    history = model.fit(
        X_train_scaled, y_train,
        epochs=50,
        batch_size=32,
        validation_split=0.2,
        verbose=1
    )
    
    # è¯„ä¼°
    test_loss, test_accuracy = model.evaluate(X_test_scaled, y_test, verbose=0)
    y_pred = model.predict(X_test_scaled)
    y_pred_classes = np.argmax(y_pred, axis=1)

    print(f"\nâœ… è®­ç»ƒå®Œæˆ!")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")

    # åˆ†ç±»æŠ¥å‘Š
    if class_names is None:
        unique_eval_classes = np.unique(np.concatenate([y_train, y_test]))
        class_names = [f'class_{idx}' for idx in unique_eval_classes]
    unique_eval_classes = np.unique(y_test)
    textual_report = classification_report(
        y_test,
        y_pred_classes,
        labels=unique_eval_classes,
        target_names=[class_names[i] for i in unique_eval_classes],
        digits=4,
    )
    report_dict = classification_report(
        y_test,
        y_pred_classes,
        labels=unique_eval_classes,
        target_names=[class_names[i] for i in unique_eval_classes],
        digits=4,
        output_dict=True,
    )
    conf_matrix = confusion_matrix(y_test, y_pred_classes)

    print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
    print(textual_report)

    return model, scaler, test_accuracy, history, textual_report, report_dict, conf_matrix


def train_cnn_model(
    train_sequence,
    val_sequence,
    test_sequence,
    class_names,
    epochs=40,
    class_weight_strategy='balanced',
    augmentation_strategy=None,
):
    """ä½¿ç”¨CNNè®­ç»ƒåŸºäºå°æ³¢å¼ é‡çš„æ¨¡å‹ï¼Œå¹¶æä¾›æ›´ä¸°å¯Œçš„è°ƒè¯•ä¿¡æ¯"""

    print("ğŸ§  è®­ç»ƒCNNæ¨¡å‹...")

    train_distribution = train_sequence.get_base_distribution(class_names)
    val_distribution = val_sequence.get_base_distribution(class_names)
    test_distribution = test_sequence.get_base_distribution(class_names)

    print("   ğŸ“ˆ è®­ç»ƒé›†ç±»åˆ«åˆ†å¸ƒ:")
    for name, count in train_distribution.items():
        print(f"      - {name}: {count}")

    print("   ğŸ“Š éªŒè¯é›†ç±»åˆ«åˆ†å¸ƒ:")
    for name, count in val_distribution.items():
        print(f"      - {name}: {count}")

    print("   ğŸ“¦ æµ‹è¯•é›†ç±»åˆ«åˆ†å¸ƒ:")
    for name, count in test_distribution.items():
        print(f"      - {name}: {count}")

    augmentations = train_sequence.get_augmentations()
    balanced_distribution = train_sequence.get_balanced_distribution(class_names)

    if augmentations:
        print("   â™»ï¸  å¯¹ä»¥ä¸‹ç±»åˆ«æ‰§è¡Œäº†è¿‡é‡‡æ ·:")
        for label_idx, stats in augmentations.items():
            class_name = class_names[label_idx] if 0 <= label_idx < len(class_names) else str(label_idx)
            print(
                f"      - {class_name}: åŸå§‹ {stats['original']} ä¸ª â†’ å¢è¡¥ {stats['added']} ä¸ª â†’ æœ€ç»ˆ {stats['final']} ä¸ª"
            )

    if balanced_distribution:
        print("   ğŸ”„ è¿‡é‡‡æ ·åè®­ç»ƒé›†åˆ†å¸ƒ:")
        for name, count in balanced_distribution.items():
            print(f"      - {name}: {count}")
        if train_sequence.augment:
            print("   ï¼ˆå°æ³¢å¼ é‡åœ¨æ‰¹æ¬¡ä¸­åº”ç”¨éšæœºå¹³ç§»/ç¼©æ”¾/å™ªå£°å¢å¼ºï¼‰")

    num_classes = len(class_names)
    model = build_cnn_model(train_sequence.sample_shape, num_classes)

    base_labels = train_sequence.get_labels()
    unique_classes = np.unique(base_labels)
    unique_classes = np.sort(unique_classes)
    class_weight_dict = {}
    if class_weight_strategy:
        weights = class_weight.compute_class_weight(
            class_weight=class_weight_strategy,
            classes=unique_classes,
            y=base_labels,
        )
        class_weight_dict = {int(cls): float(weight) for cls, weight in zip(unique_classes, weights)}

    callbacks = [
        EarlyStopping(monitor='val_loss', patience=6, restore_best_weights=True),
        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-5),
    ]

    history = model.fit(
        train_sequence,
        epochs=epochs,
        validation_data=val_sequence,
        class_weight=class_weight_dict if class_weight_dict else None,
        callbacks=callbacks,
        verbose=1,
    )

    val_metrics = model.evaluate(val_sequence, verbose=0, return_dict=True)
    test_metrics = model.evaluate(test_sequence, verbose=0, return_dict=True)

    val_loss = float(val_metrics.get('loss', 0.0))
    val_accuracy = float(val_metrics.get('accuracy', 0.0))
    val_top3 = float(val_metrics.get('top3_accuracy', 0.0))

    test_loss = float(test_metrics.get('loss', 0.0))
    test_accuracy = float(test_metrics.get('accuracy', 0.0))
    test_top3 = float(test_metrics.get('top3_accuracy', 0.0))
    y_pred = model.predict(test_sequence, verbose=0)
    y_pred_classes = np.argmax(y_pred, axis=1)
    y_test = test_sequence.get_labels()

    print(f"\nâœ… CNNè®­ç»ƒå®Œæˆ!")
    print(f"   éªŒè¯å‡†ç¡®ç‡: {val_accuracy:.4f} (Top-3={val_top3:.4f}, loss={val_loss:.4f})")
    print(f"   æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f} (Top-3={test_top3:.4f}, loss={test_loss:.4f})")

    unique_eval_classes = np.unique(y_test)
    target_names = [class_names[idx] for idx in unique_eval_classes]
    textual_report = classification_report(
        y_test,
        y_pred_classes,
        labels=unique_eval_classes,
        target_names=target_names,
        digits=4,
    )
    report_dict = classification_report(
        y_test,
        y_pred_classes,
        labels=unique_eval_classes,
        target_names=target_names,
        digits=4,
        output_dict=True,
    )
    conf_matrix = confusion_matrix(y_test, y_pred_classes)

    print("\nğŸ“Š åˆ†ç±»æŠ¥å‘Š:")
    print(textual_report)

    evaluation_summary = {
        'val_loss': val_loss,
        'val_accuracy': val_accuracy,
        'val_top3_accuracy': val_top3,
        'test_loss': test_loss,
        'test_accuracy': test_accuracy,
        'test_top3_accuracy': test_top3,
        'train_distribution': train_distribution,
        'validation_distribution': val_distribution,
        'test_distribution': test_distribution,
        'class_weight': class_weight_dict,
        'augmentations': augmentations,
        'effective_train_size': train_sequence.get_effective_size(),
        'original_train_size': train_sequence.get_original_size(),
        'balanced_train_distribution': balanced_distribution,
        'augmentation_strategy': augmentation_strategy or {
            'noise_std': train_sequence.noise_std,
            'max_time_shift': train_sequence.max_shift,
            'scale_range': list(train_sequence.scale_range) if isinstance(train_sequence.scale_range, (list, tuple)) else train_sequence.scale_range,
            'adaptive_target': train_sequence.adaptive_target,
            'target_min_samples': train_sequence.min_samples_per_class,
        },
    }

    return model, evaluation_summary, history, textual_report, report_dict, conf_matrix

def quantize_model_for_fpga(model, representative_data, output_dir, timestamp, calibration_size=256):
    """ä½¿ç”¨TensorFlow Liteè¿›è¡Œæ•´å‹é‡åŒ–ï¼Œä¾¿äºåœ¨Pynq-Z2ä¸Šéƒ¨ç½²"""

    print("âš¡ æ¨¡å‹é‡åŒ– (TensorFlow Lite INT8)...")

    if representative_data is None or len(representative_data) == 0:
        raise ValueError("ä»£è¡¨æ€§æ•°æ®é›†ä¸ºç©ºï¼Œæ— æ³•æ‰§è¡Œé‡åŒ–")

    os.makedirs(output_dir, exist_ok=True)

    representative_data = np.asarray(representative_data, dtype=np.float32)
    calibration_size = min(calibration_size, representative_data.shape[0])
    calibration_samples = representative_data[:calibration_size]

    def representative_dataset():
        for sample in calibration_samples:
            sample = sample.astype(np.float32)
            yield [np.expand_dims(sample, axis=0)]

    converter = tf.lite.TFLiteConverter.from_keras_model(model)
    converter.optimizations = [tf.lite.Optimize.DEFAULT]
    converter.representative_dataset = representative_dataset
    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]
    converter.inference_input_type = tf.int8
    converter.inference_output_type = tf.int8

    quant_mode = 'int8'
    try:
        tflite_model = converter.convert()
    except Exception as exc:
        print(f"   âš ï¸ INT8é‡åŒ–å¤±è´¥ ({exc})ï¼Œå°è¯•Float16é‡åŒ–ä½œä¸ºå›é€€æ–¹æ¡ˆ")
        converter = tf.lite.TFLiteConverter.from_keras_model(model)
        converter.optimizations = [tf.lite.Optimize.DEFAULT]
        converter.target_spec.supported_types = [tf.float16]
        quant_mode = 'float16'
        tflite_model = converter.convert()

    quant_model_path = os.path.join(output_dir, f"ecg_cnn_{quant_mode}_{timestamp}.tflite")
    with open(quant_model_path, 'wb') as f:
        f.write(tflite_model)

    quantization_details = {
        'mode': quant_mode,
        'calibration_samples': int(calibration_size),
        'tflite_path': quant_model_path,
        'tflite_size_bytes': len(tflite_model)
    }

    print(f"   âœ… é‡åŒ–å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {quant_model_path}")

    return quant_model_path, quantization_details



def export_cnn_weights_for_hls(model, output_dir, fixed_point_total_bits=16, fixed_point_integer_bits=6):
    """å¯¼å‡ºCNNæƒé‡åˆ°HLSæ¨¡æ¿æ‰€éœ€çš„å¤´æ–‡ä»¶ä¸NPZæƒé‡åŒ…"""

    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    if len(model.input_shape) != 4:
        raise ValueError("æ¨¡å‹è¾“å…¥å¿…é¡»æ˜¯å››ç»´å¼ é‡ [B, H, W, C]")

    input_height, input_width, input_channels = [int(dim) for dim in model.input_shape[1:]]

    conv_layers = []
    dense_layers = []
    flatten_size = None

    for layer in model.layers:
        if isinstance(layer, Conv2D):
            weights, biases = layer.get_weights()
            weights = np.transpose(weights.astype(np.float32), (3, 2, 0, 1))  # OC, IC, KH, KW
            conv_layers.append({
                'name': layer.name,
                'weights': weights,
                'biases': biases.astype(np.float32),
                'kernel': (int(weights.shape[2]), int(weights.shape[3])),
                'in_channels': int(weights.shape[1]),
                'out_channels': int(weights.shape[0]),
                'output_shape': [int(dim) for dim in layer.output_shape[1:4]]
            })

        elif isinstance(layer, BatchNormalization):
            if not conv_layers:
                continue
            gamma, beta, moving_mean, moving_variance = layer.get_weights()
            epsilon = getattr(layer, 'epsilon', 1e-3)
            scale = gamma / np.sqrt(moving_variance + epsilon)
            offset = beta - moving_mean * scale
            conv_layers[-1]['bn_scale'] = scale.astype(np.float32)
            conv_layers[-1]['bn_offset'] = offset.astype(np.float32)

        elif isinstance(layer, MaxPooling2D):
            if conv_layers:
                conv_layers[-1]['pool_output_shape'] = [
                    int(dim) if dim is not None else None for dim in layer.output_shape[1:4]
                ]

        elif isinstance(layer, Flatten):
            flatten_size = int(np.prod([dim for dim in layer.output_shape[1:] if dim is not None]))

        elif isinstance(layer, Dense):
            weights, biases = layer.get_weights()
            dense_layers.append({
                'name': layer.name,
                'weights': weights.astype(np.float32).T,  # OUT, IN
                'biases': biases.astype(np.float32),
                'units': int(layer.units),
                'input_dim': int(weights.shape[0]),
                'activation': getattr(layer.activation, '__name__', 'linear')
            })

    if not conv_layers or not dense_layers:
        raise ValueError("æ¨¡å‹ä¸­æœªæ‰¾åˆ°å·ç§¯æˆ–å…¨è¿æ¥å±‚ï¼Œæ— æ³•å¯¼å‡ºHLSæƒé‡")

    if len(conv_layers) != 3:
        raise ValueError(f"å½“å‰HLSæ¨¡æ¿å‡å®š3ä¸ªå·ç§¯å—ï¼Œæ£€æµ‹åˆ° {len(conv_layers)} ä¸ªã€‚è¯·è°ƒæ•´æ¨¡å‹æˆ–æ‰©å±•æ¨¡æ¿ã€‚")
    if len(dense_layers) < 2:
        raise ValueError(f"å½“å‰HLSæ¨¡æ¿å‡å®šè‡³å°‘2ä¸ªå…¨è¿æ¥å±‚ï¼Œæ£€æµ‹åˆ° {len(dense_layers)} ä¸ªã€‚")

    for conv in conv_layers:
        if 'bn_scale' not in conv:
            conv['bn_scale'] = np.ones((conv['out_channels'],), dtype=np.float32)
            conv['bn_offset'] = np.zeros((conv['out_channels'],), dtype=np.float32)
        if 'pool_output_shape' not in conv:
            conv['pool_output_shape'] = conv['output_shape']

    if flatten_size is None:
        raise ValueError("æ¨¡å‹æœªåŒ…å«Flattenå±‚ï¼Œæ— æ³•ç¡®å®šå…¨è¿æ¥è¾“å…¥ç»´åº¦")

    def format_array(name, values, values_per_line=8):
        values = np.asarray(values, dtype=np.float32).ravel()
        lines = [f"static const float {name}[{values.size}] = {{"]
        for start in range(0, values.size, values_per_line):
            chunk = values[start:start + values_per_line]
            chunk_str = ", ".join(f"{val:.8e}f" for val in chunk)
            suffix = ',' if start + values_per_line < values.size else ''
            lines.append(f"    {chunk_str}{suffix}")
        lines.append("};")
        lines.append("")
        return lines

    num_classes = int(dense_layers[-1]['units'])

    header_lines = [
        "#ifndef CNN_WEIGHTS_H",
        "#define CNN_WEIGHTS_H",
        "",
        "#include <cstddef>",
        "",
        f"static constexpr int CNN_HLS_TOTAL_BITS = {int(fixed_point_total_bits)};",
        f"static constexpr int CNN_HLS_INTEGER_BITS = {int(fixed_point_integer_bits)};",
        "",
        f"static constexpr int INPUT_HEIGHT = {input_height};",
        f"static constexpr int INPUT_WIDTH = {input_width};",
        f"static constexpr int INPUT_CHANNELS = {input_channels};",
        f"static constexpr int NUM_CLASSES = {num_classes};",
        ""
    ]

    hls_manifest = {
        'input_shape': [input_height, input_width, input_channels],
        'conv_layers': [],
        'dense_layers': [],
        'flatten_size': int(flatten_size),
        'num_classes': num_classes,
        'weight_statistics': {
            'conv_layers': [],
            'dense_layers': []
        }
    }

    npz_tensors = {}

    for idx, conv in enumerate(conv_layers, start=1):
        kernel_h, kernel_w = conv['kernel']
        out_h, out_w, out_c = conv['output_shape'][0], conv['output_shape'][1], conv['output_shape'][2]
        pool_h, pool_w, pool_c = conv['pool_output_shape'][0], conv['pool_output_shape'][1], conv['pool_output_shape'][2]

        header_lines.extend([
            f"// Conv block {idx}: {conv['name']}",
            f"static constexpr int CONV{idx}_KERNEL_H = {kernel_h};",
            f"static constexpr int CONV{idx}_KERNEL_W = {kernel_w};",
            f"static constexpr int CONV{idx}_IN_CHANNELS = {conv['in_channels']};",
            f"static constexpr int CONV{idx}_OUT_CHANNELS = {conv['out_channels']};",
            f"static constexpr int CONV{idx}_OUTPUT_HEIGHT = {out_h};",
            f"static constexpr int CONV{idx}_OUTPUT_WIDTH = {out_w};",
            f"static constexpr int POOL{idx}_OUTPUT_HEIGHT = {pool_h};",
            f"static constexpr int POOL{idx}_OUTPUT_WIDTH = {pool_w};",
            f"static constexpr int POOL{idx}_OUTPUT_CHANNELS = {pool_c};",
            ""
        ])

        weight_key = f"conv{idx}_weights"
        bias_key = f"conv{idx}_biases"
        scale_key = f"conv{idx}_bn_scale"
        offset_key = f"conv{idx}_bn_offset"

        npz_tensors[weight_key] = conv['weights']
        npz_tensors[bias_key] = conv['biases']
        npz_tensors[scale_key] = conv['bn_scale']
        npz_tensors[offset_key] = conv['bn_offset']

        header_lines.extend(format_array(f"CONV{idx}_WEIGHTS", conv['weights']))
        header_lines.extend(format_array(f"CONV{idx}_BIASES", conv['biases']))
        header_lines.extend(format_array(f"CONV{idx}_BN_SCALE", conv['bn_scale']))
        header_lines.extend(format_array(f"CONV{idx}_BN_OFFSET", conv['bn_offset']))

        hls_manifest['conv_layers'].append({
            'name': conv['name'],
            'kernel': [kernel_h, kernel_w],
            'in_channels': conv['in_channels'],
            'out_channels': conv['out_channels'],
            'output_shape': conv['output_shape'],
            'pool_output_shape': conv['pool_output_shape']
        })

        hls_manifest['weight_statistics']['conv_layers'].append({
            'name': conv['name'],
            'weights': {
                'min': float(np.min(conv['weights'])),
                'max': float(np.max(conv['weights'])),
                'mean': float(np.mean(conv['weights'])),
                'std': float(np.std(conv['weights']))
            },
            'biases': {
                'min': float(np.min(conv['biases'])),
                'max': float(np.max(conv['biases'])),
                'mean': float(np.mean(conv['biases'])),
                'std': float(np.std(conv['biases']))
            },
            'batch_norm': {
                'scale_min': float(np.min(conv['bn_scale'])),
                'scale_max': float(np.max(conv['bn_scale'])),
                'offset_min': float(np.min(conv['bn_offset'])),
                'offset_max': float(np.max(conv['bn_offset']))
            }
        })

    header_lines.append(f"static constexpr int FLATTEN_SIZE = {flatten_size};")
    header_lines.append("")

    for idx, dense in enumerate(dense_layers, start=1):
        npz_tensors[f"dense{idx}_weights"] = dense['weights']
        npz_tensors[f"dense{idx}_biases"] = dense['biases']

        header_lines.extend([
            f"// Dense layer {idx}: {dense['name']} ({dense['activation']})",
            f"static constexpr int DENSE{idx}_INPUTS = {dense['input_dim']};",
            f"static constexpr int DENSE{idx}_OUTPUTS = {dense['units']};",
            ""
        ])
        header_lines.extend(format_array(f"DENSE{idx}_WEIGHTS", dense['weights']))
        header_lines.extend(format_array(f"DENSE{idx}_BIASES", dense['biases']))

        hls_manifest['dense_layers'].append({
            'name': dense['name'],
            'inputs': dense['input_dim'],
            'units': dense['units'],
            'activation': dense['activation']
        })

        hls_manifest['weight_statistics']['dense_layers'].append({
            'name': dense['name'],
            'weights': {
                'min': float(np.min(dense['weights'])),
                'max': float(np.max(dense['weights'])),
                'mean': float(np.mean(dense['weights'])),
                'std': float(np.std(dense['weights']))
            },
            'biases': {
                'min': float(np.min(dense['biases'])),
                'max': float(np.max(dense['biases'])),
                'mean': float(np.mean(dense['biases'])),
                'std': float(np.std(dense['biases']))
            }
        })

    header_lines.append("#endif // CNN_WEIGHTS_H")

    header_path = output_path / "cnn_weights.h"
    header_path.write_text("\n".join(header_lines), encoding='utf-8')

    weights_npz_path = output_path / "cnn_weights.npz"
    np.savez_compressed(weights_npz_path, **npz_tensors)

    with open(output_path / "hls_manifest.json", 'w', encoding='utf-8') as f:
        json.dump(hls_manifest, f, indent=2, ensure_ascii=False)

    print(f"âœ… CNNæƒé‡å·²å¯¼å‡º: {weights_npz_path}")
    return str(weights_npz_path), str(header_path), hls_manifest


def create_fpga_deployment_package(model,
                                   class_names,
                                   label_mapping,
                                   quant_model_path,
                                   quantization_details,
                                   history,
                                   textual_report,
                                   conf_matrix,
                                   class_distribution,
                                   timestamp,
                                   per_class_metrics=None,
                                   excluded_classes=None,
                                   split_distributions=None,
                                   validation_metrics=None,
                                   wavelet_band_info=None,
                                   cwt_settings=None):
    """åˆ›å»ºé€‚é…Pynq-Z2çš„éƒ¨ç½²èµ„æºåŒ…"""

    print("\nğŸ”§ åˆ›å»ºFPGA/Pynqéƒ¨ç½²èµ„æºåŒ…...")

    output_dir = Path(f"outputs/fpga_deployment_{timestamp}")
    output_dir.mkdir(parents=True, exist_ok=True)

    quant_path = Path(quant_model_path)
    quant_dest = output_dir / quant_path.name
    shutil.copy2(quant_path, quant_dest)

    weights_npz_path, weights_header_path, hls_manifest = export_cnn_weights_for_hls(model, output_dir / "weights")
    weights_npz_path = Path(weights_npz_path)
    weights_header_path = Path(weights_header_path)
    weights_dir = weights_npz_path.parent

    history_data = {}
    if history is not None and hasattr(history, 'history'):
        history_data = {k: [float(x) for x in v] for k, v in history.history.items()}

    metadata = {
        'model_type': 'Wavelet-CNN (CWT + 2D CNN)',
        'timestamp': timestamp,
        'input_shape': list(model.input_shape[1:]),
        'num_parameters': int(model.count_params()),
        'class_names': class_names,
        'label_mapping': label_mapping,
        'class_distribution': class_distribution,
        'quantization': quantization_details,
        'training_history': history_data,
        'confusion_matrix': conf_matrix.tolist(),
        'weights_npz': os.path.relpath(weights_npz_path, output_dir),
        'weights_header': os.path.relpath(weights_header_path, output_dir),
        'hls_manifest': os.path.relpath(weights_dir / "hls_manifest.json", output_dir),
        'weight_statistics': hls_manifest.get('weight_statistics', {}),
        'tflite_model': quant_dest.name,
        'per_class_metrics': per_class_metrics or {},
        'excluded_classes': excluded_classes or {}
    }

    if validation_metrics is not None:
        metadata['validation_metrics'] = validation_metrics

    if split_distributions is not None:
        metadata['dataset_split_distribution'] = split_distributions

    if wavelet_band_info is not None:
        metadata['wavelet_bands'] = wavelet_band_info

    if cwt_settings is not None:
        metadata['cwt_settings'] = cwt_settings

    with open(output_dir / "deployment_metadata.json", 'w', encoding='utf-8') as f:
        json.dump(metadata, f, indent=2, ensure_ascii=False)

    with open(output_dir / "classification_report.txt", 'w', encoding='utf-8') as f:
        f.write(textual_report + "\n")

    np.save(output_dir / "confusion_matrix.npy", conf_matrix)

    readme_text = textwrap.dedent(f"""
        # Pynq-Z2 å¿ƒå¾‹å¤±å¸¸CNNéƒ¨ç½²æŒ‡å—

        æœ¬ç›®å½•åŒ…å«åŸºäºè¿ç»­å°æ³¢å˜æ¢ (CWT) + å·ç§¯ç¥ç»ç½‘ç»œ (CNN) çš„å¿ƒå¾‹å¤±å¸¸åˆ†ç±»æ¨¡å‹ï¼Œå·²ç»å®ŒæˆINT8é‡åŒ–ï¼Œæ”¯æŒç›´æ¥åœ¨ Pynq-Z2 çš„ ARM ç«¯é€šè¿‡ TensorFlow Lite è¿è¡Œï¼Œæˆ–è¿›ä¸€æ­¥ç§»æ¤åˆ° DPU/HLS åŠ é€Ÿå™¨ä¸­ã€‚

        ## ç›®å½•ç»“æ„
        - `{quant_dest.name}`: é‡åŒ–åçš„ TensorFlow Lite æ¨¡å‹ã€‚
        - `weights/cnn_weights.npz`: åŸå§‹æµ®ç‚¹å·ç§¯/å…¨è¿æ¥æƒé‡ï¼Œä¾¿äºå®šåˆ¶åŒ–é‡åŒ–æˆ–FINN/TVMç­‰å·¥å…·é“¾ä½¿ç”¨ã€‚
        - `weights/cnn_weights.h`: HLS å‹å¥½çš„æƒé‡å¤´æ–‡ä»¶ï¼Œå¯ç›´æ¥åœ¨Vitis HLSé¡¹ç›®ä¸­åŒ…å«ã€‚
        - `hls/`: ç»“åˆ `cnn_weights.h` çš„Vitis HLSæ¨ç†æ¨¡æ¿æºç ï¼Œå¯ç›´æ¥ç»¼åˆç”ŸæˆPynq-Z2åŠ é€Ÿæ ¸ã€‚
        - `deployment_metadata.json`: æ¨¡å‹ç»“æ„ã€é‡åŒ–ã€ç±»åˆ«æ˜ å°„ç­‰å…³é”®ä¿¡æ¯ã€‚
        - `classification_report.txt`: æµ‹è¯•é›†åˆ†ç±»æŒ‡æ ‡ã€‚
        - `confusion_matrix.npy`: æµ‹è¯•é›†æ··æ·†çŸ©é˜µ (numpyæ ¼å¼)ã€‚
        - `pynq_z2_tflite_inference.py`: Pynq-Z2 ä¸Šçš„æ¨ç†ç¤ºä¾‹è„šæœ¬ã€‚

        ## åœ¨Pynq-Z2ä¸Šè¿è¡ŒTensorFlow Lite
        1. å°†æ•´ä¸ª `fpga_deployment_{timestamp}` ç›®å½•å¤åˆ¶åˆ°æ¿å¡ï¼ˆä¾‹å¦‚ `/home/xilinx/ecg_cnn`ï¼‰ã€‚
        2. åœ¨Pynqç»ˆç«¯æ‰§è¡Œ `sudo pip3 install --upgrade tflite-runtime pywavelets numpy` å®‰è£…ä¾èµ–ã€‚
        3. è¿›å…¥ç›®å½•å¹¶è¿è¡Œ `python3 pynq_z2_tflite_inference.py --input sample_beat.npy`ï¼Œè„šæœ¬ä¼šè‡ªåŠ¨ç”Ÿæˆå°æ³¢å°ºåº¦å›¾å¹¶è°ƒç”¨é‡åŒ–æ¨¡å‹è¾“å‡ºé¢„æµ‹ç»“æœã€‚

        ## åœ¨FPGA/DPUä¸Šè¿›ä¸€æ­¥åŠ é€Ÿ
        - ä½¿ç”¨ `weights/cnn_weights.npz` ä¸ `deployment_metadata.json` ä¸­çš„è¾“å…¥å½¢çŠ¶ã€é€šé“é¡ºåºä¿¡æ¯ï¼Œå¯åœ¨Vitis AIæˆ–FINNå·¥å…·é“¾ä¸­é‡å»ºå¹¶é‡åŒ–ç½‘ç»œã€‚
        - `weights/cnn_weights.h` å¯ç›´æ¥åŒ…å«åˆ°Vitis HLSé¡¹ç›®ä¸­ï¼Œç»“åˆ `deployment_metadata.json` çš„é‡åŒ–æ¯”ä¾‹å®ç°æ‰‹å†™åŠ é€Ÿå™¨ã€‚

        ## è¾“å…¥é¢„å¤„ç†
        - è¾“å…¥ä¸ºå•å¯¼è” ECG å¿ƒæ‹ï¼ˆ300æ ·æœ¬ï¼Œ360Hzé‡‡æ ·ï¼‰ã€‚
        - é¢„å¤„ç†ä¸è®­ç»ƒä¿æŒä¸€è‡´ï¼šå¸¦é€šæ»¤æ³¢ â†’ å°æ³¢CWT (`morl`, 1~64å°ºåº¦) â†’ å¹…å€¼å½’ä¸€åŒ–è‡³[0,1] â†’ æ„é€ åŒ…å«ä½é¢‘/ä¸­é¢‘/é«˜é¢‘ä¸‰ä¸ªé€šé“çš„CNNè¾“å…¥ (HÃ—WÃ—3)ã€‚

        ## æ”¯æŒçš„å¿ƒå¾‹ç±»å‹
        {', '.join(class_names)}

        å¦‚éœ€é›†æˆåˆ°è‡ªå®šä¹‰å·¥ç¨‹ï¼Œå¯å‚è€ƒ `pynq_z2_tflite_inference.py` äº†è§£å®Œæ•´çš„æ•°æ®æµã€‚
    """)

    with open(output_dir / "README.md", 'w', encoding='utf-8') as f:
        f.write(readme_text)

    hls_template_dir = Path('FPGA/hls_cnn')
    if hls_template_dir.exists():
        target_hls_dir = output_dir / 'hls'
        shutil.copytree(hls_template_dir, target_hls_dir, dirs_exist_ok=True)
        print(f"   âœ… å·²å¤åˆ¶HLSæ¨ç†æ¨¡æ¿: {target_hls_dir}")

    pynq_script = textwrap.dedent("""
        # Pynq-Z2 TensorFlow Lite æ¨ç†ç¤ºä¾‹

        import argparse
        import json
        from pathlib import Path

        import numpy as np
        import pywt
        from tflite_runtime.interpreter import Interpreter


        def create_wavelet_scalogram(beat, wavelet='morl', scales=None):
            if scales is None:
                scales = np.arange(1, 65)
            beat = np.asarray(beat, dtype=np.float32)
            beat = (beat - np.mean(beat)) / (np.std(beat) + 1e-8)
            coefficients, _ = pywt.cwt(beat, scales, wavelet)
            scalogram = np.abs(coefficients).astype(np.float32)
            min_val = scalogram.min()
            max_val = scalogram.max()
            scalogram = (scalogram - min_val) / (max_val - min_val + 1e-8)
            return scalogram


        def load_metadata(base_dir):
            with open(base_dir / "deployment_metadata.json", 'r', encoding='utf-8') as f:
                return json.load(f)


        def run_inference(base_dir, beat_path):
            metadata = load_metadata(base_dir)
            interpreter = Interpreter(model_path=str(base_dir / metadata['tflite_model']))
            interpreter.allocate_tensors()

            input_details = interpreter.get_input_details()[0]
            output_details = interpreter.get_output_details()[0]

            if beat_path is None:
                raise ValueError('è¯·æä¾›åŒ…å«å•ä¸ªå¿ƒæ‹æ³¢å½¢ (300æ ·æœ¬) çš„ .npy æ–‡ä»¶')

            beat = np.load(beat_path)
            scalogram = create_wavelet_scalogram(beat)
            scalogram = scalogram[..., np.newaxis]

            scale, zero_point = input_details['quantization']
            if scale == 0:
                raise RuntimeError('é‡åŒ–æ¯”ä¾‹ä¸º0ï¼Œè¯·æ£€æŸ¥é‡åŒ–æ¨¡å‹ã€‚')
            quantized = np.round(scalogram / scale + zero_point)
            quantized = np.clip(quantized, -128, 127).astype(np.int8)
            quantized = quantized[np.newaxis, ...]

            interpreter.set_tensor(input_details['index'], quantized)
            interpreter.invoke()
            prediction = interpreter.get_tensor(output_details['index'])[0]

            predicted_idx = int(np.argmax(prediction))
            class_names = metadata['class_names']
            print("é¢„æµ‹ç±»åˆ«:", class_names[predicted_idx])
            print("å„ç±»åˆ«æ¦‚ç‡:")
            for name, prob in zip(class_names, prediction):
                print(f"  {name}: {prob:.4f}")


        def main():
            parser = argparse.ArgumentParser(description='Pynq-Z2 ECG CNN inference demo')
            parser.add_argument('--input', required=True, help='åŒ…å«å•ä¸ªå¿ƒæ‹ (300æ ·æœ¬) çš„ .npy æ–‡ä»¶è·¯å¾„')
            args = parser.parse_args()

            base_dir = Path(__file__).resolve().parent
            run_inference(base_dir, Path(args.input))


        if __name__ == '__main__':
            main()
    """)

    with open(output_dir / "pynq_z2_tflite_inference.py", 'w', encoding='utf-8') as f:
        f.write(pynq_script)

    print(f"âœ… FPGAéƒ¨ç½²åŒ…å·²åˆ›å»º: {output_dir}")
    return str(output_dir), hls_manifest.get('weight_statistics', {})


def _collect_per_class_metrics(report_dict):
    metrics = {}
    for class_name, values in report_dict.items():
        if class_name in {'accuracy', 'macro avg', 'weighted avg'}:
            continue
        metrics[class_name] = {
            'precision': float(values.get('precision', 0.0)),
            'recall': float(values.get('recall', 0.0)),
            'f1_score': float(values.get('f1-score', 0.0)),
            'support': int(values.get('support', 0)),
        }
    return metrics


def _format_top_classes(metrics_dict, top_k=5):
    if not metrics_dict:
        return ""
    sorted_items = sorted(metrics_dict.items(), key=lambda kv: kv[1]['f1_score'], reverse=True)
    lines = []
    for name, stats in sorted_items[:top_k]:
        lines.append(
            f"      â€¢ {name}: F1={stats['f1_score']:.4f}, ç²¾ç¡®ç‡={stats['precision']:.4f}, å¬å›ç‡={stats['recall']:.4f}, æ ·æœ¬æ•°={stats['support']}"
        )
    return "\n".join(lines)


def _format_weight_summary(weight_stats):
    if not weight_stats:
        return ""
    lines = []
    for layer in weight_stats.get('conv_layers', []):
        w = layer['weights']
        b = layer['biases']
        lines.append(
            f"      â€¢ {layer['name']} å·ç§¯æƒé‡èŒƒå›´[{w['min']:.4f}, {w['max']:.4f}] (Î¼={w['mean']:.4f}, Ïƒ={w['std']:.4f}); åç½®èŒƒå›´[{b['min']:.4f}, {b['max']:.4f}]"
        )
    for layer in weight_stats.get('dense_layers', []):
        w = layer['weights']
        b = layer['biases']
        lines.append(
            f"      â€¢ {layer['name']} å…¨è¿æ¥æƒé‡èŒƒå›´[{w['min']:.4f}, {w['max']:.4f}] (Î¼={w['mean']:.4f}, Ïƒ={w['std']:.4f}); åç½®èŒƒå›´[{b['min']:.4f}, {b['max']:.4f}]"
        )
    return "\n".join(lines)


def _save_scaler(scaler, path):
    np.savez_compressed(
        path,
        mean=scaler.mean_,
        scale=scaler.scale_,
        var=getattr(scaler, 'var_', np.square(scaler.scale_)),
    )


def main():
    """ä¸»ç¨‹åº - ä½¿ç”¨çœŸå®æ•°æ®è®­ç»ƒ"""

    parser = argparse.ArgumentParser(description='MIT-BIH ECG training pipeline')
    parser.add_argument(
        '--model',
        choices=['cnn', 'mlp', 'both'],
        default='cnn',
        help='é€‰æ‹©è®­ç»ƒCNNã€MLPæˆ–åŒæ—¶è®­ç»ƒäºŒè€…',
    )
    parser.add_argument(
        '--max-records',
        type=int,
        default=10,
        help='ä»MIT-BIHæ•°æ®é›†ä¸­åŠ è½½çš„è®°å½•æ•°é‡ï¼ˆé»˜è®¤10ï¼‰',
    )
    args = parser.parse_args()

    try:
        print("ğŸ“Š åŠ è½½MIT-BIHæ•°æ®åº“...")

        current_dir = os.path.dirname(os.path.abspath(__file__))
        data_path = os.path.join(current_dir, 'data')
        loader = MITBIHDataLoader(data_path)
        beats, labels, label_mapping, class_names = loader.load_all_data(max_records=args.max_records)

        beats, labels, label_mapping, class_names, dropped_classes = filter_rare_classes(
            beats,
            labels,
            label_mapping,
            class_names,
            min_count=2,
        )

        if dropped_classes:
            print("\nâš ï¸ ä»¥ä¸‹ç±»åˆ«çš„æ ·æœ¬æ•°ä¸è¶³2ä¸ªï¼Œå·²ä»æœ¬æ¬¡è®­ç»ƒä¸­ç§»é™¤ï¼š")
            for name, count in dropped_classes.items():
                print(f"   - {name}: {count} ä¸ªæ ·æœ¬")

        unique_indices, counts = np.unique(labels, return_counts=True)
        class_distribution = {class_names[idx]: int(count) for idx, count in zip(unique_indices, counts)}

        executed_models = []

        if args.model in {'cnn', 'both'}:
            print("\n=== Wavelet-CNN è®­ç»ƒæµç¨‹ ===")
            cnn_start = time.time()

            cwt_scales = np.arange(1, 65)
            _wavelet_memmap, wavelet_info = create_wavelet_tensors(
                beats,
                scales=cwt_scales,
                fs=loader.fs,
                wavelet='morl',
                channel_strategy='cardiac_band',
                cache_dir='outputs/cache',
                dtype=np.float16,
            )
            tensor_shape = tuple(wavelet_info['shape'])
            print(f"   âœ… å°æ³¢å¼ é‡å½¢çŠ¶: {tensor_shape}")
            print(f"   ğŸ’¾ å·²å†™å…¥å°æ³¢ç¼“å­˜æ–‡ä»¶: {wavelet_info['path']} (dtype={wavelet_info['dtype']})")
            print("   ğŸ¯ å°æ³¢é€šé“è¦†ç›–é¢‘æ®µ:")
            for band in CARDIAC_WAVELET_BANDS:
                print(
                    f"      - {band['name']}: {band['range_hz'][0]:.1f}-{band['range_hz'][1]:.1f} Hz ({band['description']})"
                )

            all_indices = np.arange(labels.shape[0])
            (
                train_indices,
                val_indices,
                test_indices,
                y_train,
                y_val,
                y_test,
            ) = stratified_train_val_test_split(
                all_indices,
                labels,
                test_size=0.2,
                val_size=0.1,
                random_state=42,
            )

            print(
                f"   âœ… æ•°æ®é›†åˆ’åˆ†: è®­ç»ƒ {train_indices.shape[0]} / éªŒè¯ {val_indices.shape[0]} / æµ‹è¯• {test_indices.shape[0]}"
            )

            augmentation_config = {
                'noise_std': 0.02,
                'max_time_shift': 4,
                'scale_range': (0.9, 1.1),
                'target_min_samples': 32,
                'adaptive_target': True,
            }

            train_sequence = WaveletTensorSequence(
                memmap_info=wavelet_info,
                indices=train_indices,
                labels=y_train,
                batch_size=32,
                shuffle=True,
                augment=True,
                noise_std=augmentation_config['noise_std'],
                max_shift=augmentation_config['max_time_shift'],
                scale_range=augmentation_config['scale_range'],
                oversample_target=augmentation_config['target_min_samples'],
                adaptive_target=augmentation_config['adaptive_target'],
            )
            val_sequence = WaveletTensorSequence(
                memmap_info=wavelet_info,
                indices=val_indices,
                labels=y_val,
                batch_size=32,
                shuffle=False,
                augment=False,
            )
            test_sequence = WaveletTensorSequence(
                memmap_info=wavelet_info,
                indices=test_indices,
                labels=y_test,
                batch_size=32,
                shuffle=False,
                augment=False,
            )

            del _wavelet_memmap

            (
                model,
                evaluation_summary,
                history,
                textual_report,
                report_dict,
                conf_matrix,
            ) = train_cnn_model(
                train_sequence,
                val_sequence,
                test_sequence,
                class_names=class_names,
                epochs=40,
                class_weight_strategy='balanced',
                augmentation_strategy={
                    'noise_std': augmentation_config['noise_std'],
                    'max_time_shift': augmentation_config['max_time_shift'],
                    'scale_range': list(augmentation_config['scale_range']),
                    'target_min_samples': augmentation_config['target_min_samples'],
                    'adaptive_target': augmentation_config['adaptive_target'],
                },
            )

            per_class_metrics = _collect_per_class_metrics(report_dict)

            augmentations_named = {}
            for idx, stats in evaluation_summary.get('augmentations', {}).items():
                key = class_names[idx] if 0 <= idx < len(class_names) else str(idx)
                augmentations_named[key] = stats

            timestamp = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_cnn"

            quantized_dir = Path('outputs/quantized_models')
            quantized_dir.mkdir(parents=True, exist_ok=True)
            representative_data = sample_wavelet_representatives(
                wavelet_info,
                np.concatenate([train_indices, val_indices]),
                sample_size=512,
            )
            quant_model_path, quant_details = quantize_model_for_fpga(
                model,
                representative_data,
                str(quantized_dir),
                timestamp,
            )

            cwt_settings = {
                'wavelet': 'morl',
                'scales': [int(x) for x in cwt_scales.tolist()],
                'sampling_rate': loader.fs,
                'channel_strategy': 'cardiac_band',
                'bands': _serialize_wavelet_bands(CARDIAC_WAVELET_BANDS),
                'normalization': wavelet_info.get('normalization'),
            }

            fpga_output_dir, weight_statistics = create_fpga_deployment_package(
                model=model,
                class_names=class_names,
                label_mapping=label_mapping,
                quant_model_path=quant_model_path,
                quantization_details=quant_details,
                history=history,
                textual_report=textual_report,
                conf_matrix=conf_matrix,
                class_distribution=class_distribution,
                timestamp=timestamp,
                per_class_metrics=per_class_metrics,
                excluded_classes=dropped_classes,
                split_distributions={
                    'train': evaluation_summary.get('train_distribution', {}),
                    'validation': evaluation_summary.get('validation_distribution', {}),
                    'test': evaluation_summary.get('test_distribution', {}),
                },
                validation_metrics={
                    'val_accuracy': evaluation_summary.get('val_accuracy'),
                    'val_loss': evaluation_summary.get('val_loss'),
                    'val_top3_accuracy': evaluation_summary.get('val_top3_accuracy'),
                    'test_accuracy': evaluation_summary.get('test_accuracy'),
                    'test_top3_accuracy': evaluation_summary.get('test_top3_accuracy'),
                    'test_loss': evaluation_summary.get('test_loss'),
                },
                wavelet_band_info=_serialize_wavelet_bands(CARDIAC_WAVELET_BANDS),
                cwt_settings=cwt_settings,
            )

            history_data = {}
            if history is not None and hasattr(history, 'history'):
                history_data = {k: [float(x) for x in v] for k, v in history.history.items()}

            results = {
                'timestamp': timestamp,
                'training_time': time.time() - cnn_start,
                'total_beats': int(len(beats)),
                'feature_tensor_shape': list(tensor_shape[1:]),
                'test_accuracy': float(evaluation_summary.get('test_accuracy', 0.0)),
                'test_top3_accuracy': float(evaluation_summary.get('test_top3_accuracy', 0.0)),
                'test_loss': float(evaluation_summary.get('test_loss', 0.0)),
                'validation_accuracy': float(evaluation_summary.get('val_accuracy', 0.0)),
                'validation_top3_accuracy': float(evaluation_summary.get('val_top3_accuracy', 0.0)),
                'validation_loss': float(evaluation_summary.get('val_loss', 0.0)),
                'train_distribution': evaluation_summary.get('train_distribution', {}),
                'validation_distribution': evaluation_summary.get('validation_distribution', {}),
                'test_distribution': evaluation_summary.get('test_distribution', {}),
                'balanced_train_distribution': evaluation_summary.get('balanced_train_distribution', {}),
                'augmentations': augmentations_named,
                'class_weight': evaluation_summary.get('class_weight', {}),
                'effective_train_size': evaluation_summary.get('effective_train_size'),
                'original_train_size': evaluation_summary.get('original_train_size'),
                'augmentation_strategy': evaluation_summary.get('augmentation_strategy'),
                'num_parameters': int(model.count_params()),
                'data_source': 'MIT-BIH Arrhythmia Database',
                'technology_stack': 'Continuous Wavelet Transform + 2D CNN',
                'class_names': class_names,
                'class_distribution': class_distribution,
                'label_mapping': label_mapping,
                'wavelet_bands': _serialize_wavelet_bands(CARDIAC_WAVELET_BANDS),
                'cwt_settings': cwt_settings,
                'quantization': quant_details,
                'classification_report': report_dict,
                'confusion_matrix': conf_matrix.tolist(),
                'training_history': history_data,
                'fpga_package': fpga_output_dir,
                'tflite_model': quant_model_path,
                'per_class_metrics': per_class_metrics,
                'weight_statistics': weight_statistics,
                'excluded_classes': dropped_classes,
                'wavelet_cache': {
                    'path': wavelet_info.get('path'),
                    'dtype': wavelet_info.get('dtype'),
                    'shape': tensor_shape,
                    'normalization': wavelet_info.get('normalization'),
                },
            }

            os.makedirs('outputs/experiments', exist_ok=True)
            with open(f'outputs/experiments/cnn_training_{timestamp}.json', 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)

            os.makedirs('outputs', exist_ok=True)
            model_path = f'outputs/trained_ecg_cnn_{timestamp}.h5'
            model.save(model_path)

            print("\n" + "=" * 70)
            print("âœ… Wavelet-CNN è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ“Š æ•°æ®æº: MIT-BIHå¿ƒå¾‹å¤±å¸¸æ•°æ®åº“")
            print(f"ğŸ’“ è®­ç»ƒå¿ƒæ‹æ•°: {len(beats):,}")
            print(f"ğŸ”§ å°æ³¢å¼ é‡å°ºå¯¸: {tensor_shape[1:]}")
            print(
                "ğŸ§  éªŒè¯å‡†ç¡®ç‡: "
                f"{evaluation_summary.get('val_accuracy', 0.0):.4f} (Top-3 {evaluation_summary.get('val_top3_accuracy', 0.0):.4f}) / "
                f"æµ‹è¯•å‡†ç¡®ç‡: {evaluation_summary.get('test_accuracy', 0.0):.4f} (Top-3 {evaluation_summary.get('test_top3_accuracy', 0.0):.4f})"
            )
            print(f"â±ï¸  è®­ç»ƒè€—æ—¶: {results['training_time']:.1f} ç§’")
            print(f"ğŸ“ FPGAéƒ¨ç½²åŒ…: {fpga_output_dir}")
            print(f"ğŸ’¾ æ¨¡å‹æ–‡ä»¶: {model_path}")
            print(f"ğŸ§® æ”¯æŒå¿ƒå¾‹ç±»å‹: {', '.join(class_names)}")
            if per_class_metrics:
                print("ğŸ“ˆ å„ç±»åˆ«F1è¯„åˆ†:")
                print(_format_top_classes(per_class_metrics, top_k=min(10, len(per_class_metrics))))
            if weight_statistics:
                print("âš–ï¸ æƒé‡ç»Ÿè®¡:")
                print(_format_weight_summary(weight_statistics))
            print("ğŸ¯ æ¨¡å‹å·²é€‚é…Pynq-Z2é‡åŒ–éƒ¨ç½²æµç¨‹ï¼Œå¯ç›´æ¥å¤åˆ¶éƒ¨ç½²ç›®å½•è¿›è¡ŒéªŒè¯ã€‚")

            executed_models.append('cnn')

        if args.model in {'mlp', 'both'}:
            print("\n=== Wavelet+Time ç‰¹å¾ + MLP è®­ç»ƒæµç¨‹ ===")
            mlp_start = time.time()

            feature_vectors = extract_all_features(beats)
            print(f"   âœ… ç‰¹å¾å‘é‡å½¢çŠ¶: {feature_vectors.shape}")

            X_train, X_test, y_train, y_test = train_test_split(
                feature_vectors, labels, test_size=0.2, random_state=42, stratify=labels
            )
            print(f"   âœ… è®­ç»ƒé›†: {X_train.shape[0]}, æµ‹è¯•é›†: {X_test.shape[0]}")

            (
                model,
                scaler,
                test_accuracy,
                history,
                textual_report,
                report_dict,
                conf_matrix,
            ) = train_model(X_train, X_test, y_train, y_test, class_names=class_names)

            per_class_metrics = _collect_per_class_metrics(report_dict)

            timestamp = f"{datetime.now().strftime('%Y%m%d_%H%M%S')}_mlp"

            history_data = {}
            if history is not None and hasattr(history, 'history'):
                history_data = {k: [float(x) for x in v] for k, v in history.history.items()}

            scaler_path = Path(f'outputs/mlp_scaler_{timestamp}.npz')
            scaler_path.parent.mkdir(parents=True, exist_ok=True)
            _save_scaler(scaler, scaler_path)

            results = {
                'timestamp': timestamp,
                'training_time': time.time() - mlp_start,
                'total_beats': int(len(beats)),
                'feature_vector_dim': int(feature_vectors.shape[1]),
                'test_accuracy': float(test_accuracy),
                'num_parameters': int(model.count_params()),
                'data_source': 'MIT-BIH Arrhythmia Database',
                'technology_stack': 'Wavelet Statistical Features + Time-domain Features + MLP',
                'class_names': class_names,
                'class_distribution': class_distribution,
                'label_mapping': label_mapping,
                'classification_report': report_dict,
                'confusion_matrix': conf_matrix.tolist(),
                'training_history': history_data,
                'scaler_path': str(scaler_path),
                'per_class_metrics': per_class_metrics,
                'excluded_classes': dropped_classes,
            }

            os.makedirs('outputs/experiments', exist_ok=True)
            with open(f'outputs/experiments/mlp_training_{timestamp}.json', 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, ensure_ascii=False)

            model_path = f'outputs/trained_ecg_mlp_{timestamp}.h5'
            model.save(model_path)

            print("\n" + "=" * 70)
            print("âœ… Wavelet+Time ç‰¹å¾ + MLP è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ“Š æ•°æ®æº: MIT-BIHå¿ƒå¾‹å¤±å¸¸æ•°æ®åº“")
            print(f"ğŸ’“ è®­ç»ƒå¿ƒæ‹æ•°: {len(beats):,}")
            print(f"ğŸ§¾ ç‰¹å¾ç»´åº¦: {feature_vectors.shape[1]}")
            print(f"ğŸ§  æµ‹è¯•å‡†ç¡®ç‡: {test_accuracy:.4f}")
            print(f"â±ï¸  è®­ç»ƒè€—æ—¶: {results['training_time']:.1f} ç§’")
            print(f"ğŸ’¾ æ¨¡å‹æ–‡ä»¶: {model_path}")
            print(f"ğŸ“Š åˆ†ç±»æŠ¥å‘Šå·²å†™å…¥: outputs/experiments/mlp_training_{timestamp}.json")
            print(f"ğŸ§® æ”¯æŒå¿ƒå¾‹ç±»å‹: {', '.join(class_names)}")
            if per_class_metrics:
                print("ğŸ“ˆ å„ç±»åˆ«F1è¯„åˆ†:")
                print(_format_top_classes(per_class_metrics, top_k=min(10, len(per_class_metrics))))
            print("ğŸ¯ å·²ä¿ç•™åŸå§‹MLPç‰¹å¾å·¥ç¨‹ä¸æ¨¡å‹ï¼Œå¯ç”¨äºå¯¹æ¯”æˆ–è¿ç§»éƒ¨ç½²ã€‚")

            executed_models.append('mlp')

        if not executed_models:
            raise RuntimeError('æœªé€‰æ‹©ä»»ä½•æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚')

    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
