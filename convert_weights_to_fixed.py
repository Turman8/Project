"""
å°†æµ®ç‚¹æ•°æƒé‡è½¬æ¢ä¸ºQ8.8å®šç‚¹æ•°æ ¼å¼
"""
import numpy as np
import tensorflow as tf
import os

def float_to_q8_8(value):
    """
    å°†æµ®ç‚¹æ•°è½¬æ¢ä¸ºQ8.8å®šç‚¹æ•° (16ä½æ•´æ•°è¡¨ç¤º)
    Q8.8æ ¼å¼: 8ä½æ•´æ•°éƒ¨åˆ† + 8ä½å°æ•°éƒ¨åˆ†
    èŒƒå›´: -128.0 åˆ° +127.996
    """
    # é™åˆ¶èŒƒå›´åˆ°[-128, 127.996]
    value = np.clip(value, -128.0, 127.996)
    # è½¬æ¢ä¸ºQ8.8: ä¹˜ä»¥256 (2^8)
    return int(value * 256)

def export_fixed_point_weights():
    """å¯¼å‡ºå®šç‚¹æ•°æƒé‡åˆ°weights.cpp"""
    
    # æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
    model_files = []
    if os.path.exists('outputs'):
        for file in os.listdir('outputs'):
            if file.endswith('.h5'):
                model_files.append(os.path.join('outputs', file))
    
    if not model_files:
        print("âŒ æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
        return
    
    # ä½¿ç”¨æœ€æ–°çš„æ¨¡å‹
    latest_model = max(model_files, key=os.path.getmtime)
    print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {latest_model}")
    
    model = tf.keras.models.load_model(latest_model)
    
    # ç”Ÿæˆweights.cppæ–‡ä»¶
    with open('FPGA/hls_source/weights.cpp', 'w') as f:
        f.write('#include "weights.h"\n\n')
        f.write('// Q8.8å®šç‚¹æ•°æƒé‡æ•°æ® - ä»è®­ç»ƒæ¨¡å‹è‡ªåŠ¨ç”Ÿæˆ\n')
        f.write('// æ¯ä¸ªå€¼ä¸º16ä½æ•´æ•°ï¼Œè¡¨ç¤ºèŒƒå›´[-128.0, +127.996]\n\n')
        
        layer_names = []
        for i, layer in enumerate(model.layers):
            if hasattr(layer, 'get_weights') and layer.get_weights():
                weights = layer.get_weights()
                layer_name = f"layer{i//2 + 1}"  # è·³è¿‡dropoutå±‚
                layer_names.append(layer_name)
                
                # æƒé‡çŸ©é˜µ
                if len(weights) > 0:
                    w = weights[0]
                    w_flat = w.flatten()
                    
                    print(f"  è½¬æ¢å±‚ {layer_name}: {w.shape} -> {len(w_flat)} ä¸ªå®šç‚¹æ•°")
                    
                    f.write(f"// {layer_name} æƒé‡ {w.shape}\n")
                    f.write(f"const weight_t {layer_name}_weights[{len(w_flat)}] = {{\n")
                    
                    # è½¬æ¢ä¸ºå®šç‚¹æ•°
                    for j, val in enumerate(w_flat):
                        fixed_val = float_to_q8_8(val)
                        if j % 16 == 0:
                            f.write("    ")
                        f.write(f"{fixed_val}")
                        if j < len(w_flat) - 1:
                            f.write(", ")
                        if (j + 1) % 16 == 0 or j == len(w_flat) - 1:
                            f.write("\n")
                    f.write("};\n\n")
                
                # åç½®å‘é‡
                if len(weights) > 1:
                    b = weights[1]
                    
                    print(f"  è½¬æ¢åç½® {layer_name}: {b.shape} -> {len(b)} ä¸ªå®šç‚¹æ•°")
                    
                    f.write(f"// {layer_name} åç½® {b.shape}\n")
                    f.write(f"const weight_t {layer_name}_biases[{len(b)}] = {{\n")
                    
                    # è½¬æ¢ä¸ºå®šç‚¹æ•°
                    for j, val in enumerate(b):
                        fixed_val = float_to_q8_8(val)
                        if j % 16 == 0:
                            f.write("    ")
                        f.write(f"{fixed_val}")
                        if j < len(b) - 1:
                            f.write(", ")
                        if (j + 1) % 16 == 0 or j == len(b) - 1:
                            f.write("\n")
                    f.write("};\n\n")
        
        # æ·»åŠ æ³¨é‡Š
        f.write("/*\n")
        f.write("å®šç‚¹æ•°è½¬æ¢è¯´æ˜:\n")
        f.write("- æ ¼å¼: Q8.8 (8ä½æ•´æ•° + 8ä½å°æ•°)\n")
        f.write("- èŒƒå›´: -128.0 åˆ° +127.996\n")
        f.write("- ç²¾åº¦: 1/256 â‰ˆ 0.0039\n")
        f.write("- è½¬æ¢å…¬å¼: fixed_value = int(float_value * 256)\n")
        f.write(f"- ç”Ÿæˆæ—¶é—´: {model.summary()}\n")
        f.write("*/\n")
    
    print(f"âœ… å®šç‚¹æ•°æƒé‡å·²ç”Ÿæˆåˆ°: FPGA/hls_source/weights.cpp")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    total_params = sum([np.prod(layer.get_weights()[0].shape) + len(layer.get_weights()[1]) 
                       for layer in model.layers 
                       if hasattr(layer, 'get_weights') and layer.get_weights()])
    print(f"ğŸ“Š æ€»å‚æ•°æ•°é‡: {total_params:,}")
    print(f"ğŸ’¾ å­˜å‚¨å¤§å°: {total_params * 2:,} å­—èŠ‚ ({total_params * 2 / 1024:.1f} KB)")

if __name__ == "__main__":
    export_fixed_point_weights()
